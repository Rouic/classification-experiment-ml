{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+HXRUsgK4bzXrO1QkD0rt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rouic/classification-experiment-ml/blob/main/Test2_multi_label_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "060JksIGS-py",
        "outputId": "2ac22e53-950f-44e1-bd78-6c3de2bf6bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score, hamming_loss, precision_score, recall_score\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 2e-5  # Adjust if needed\n",
        "batch_size = 16     # Adjust if needed\n",
        "dropout_rate = 0.2   # Adjust dropout rate (0.1 - 0.5 usually work well)\n",
        "\n",
        "class TreeClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        # self.dropout = nn.Dropout(dropout_rate) # Dropout layer NOTE : not in use jet the data set is to small\n",
        "        self.fc = nn.Linear(768, num_classes)  # Adjust input size if using different embeddings\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # [CLS] token representation\n",
        "        # pooled_output = self.dropout(pooled_output) # Apply dropout NOTE : not in use jet the data set is to small\n",
        "        logits = self.fc(pooled_output)\n",
        "        probabilities = torch.sigmoid(logits)  # Apply sigmoid for multi-label classification\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "naEd4iqDTnbL"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILS\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start, end, device=None):\n",
        "    \"\"\" Print traing time \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time\n",
        "\n",
        "def getResponse(predicted_labels, threshold):\n",
        "    \"\"\" Print out message \"\"\"\n",
        "    if len(predicted_labels) == 0:\n",
        "      return f\"Im not able to confidently answer... sorry. Can you try to rephrase? (trashold: {threshold})\"\n",
        "    else:\n",
        "      return f\" \".join([label_names[i] for i in predicted_labels]) + f\" (trashold: {threshold})\""
      ],
      "metadata": {
        "id": "DeF04mQ2U2i_"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H0153lSgU8Y9",
        "outputId": "0808386b-3f71-4f0f-d580-949b4863219f"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "HCV-IaTqWvRV"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"example text 1 y\", \"example text 2 y\", \"example text 3 g\", \"example text 4 g\", \"example text 5 p\", \"example text 6 p\", \"example text 6 y\"]\n",
        "\n",
        "labels = [\n",
        "    [1, 0, 0],  # example text 1: belongs to 'y class'\n",
        "    [1, 0, 0],  # example text 2: belongs to 'y class'\n",
        "    [0, 1, 0],  # example text 3: belongs to 'g class'\n",
        "    [0, 1, 0],  # example text 4: belongs to 'g class'\n",
        "    [0, 0, 1],  # example text 5: belongs to 'p class'\n",
        "    [0, 0, 1],  # example text 6: belongs to 'p class'\n",
        "    [1, 0, 1],  # example text 6: belongs to 'y class'\n",
        "]\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "label_names = [\n",
        "    \"y class\",\n",
        "    \"g class\",\n",
        "    \"p class\"\n",
        "]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4Uw2qGO9VKxR"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataset = TextDataset(X_train, y_train, tokenizer, max_len=128)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "zoXrenWB5db4"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TextDataset(X_test, y_test, tokenizer, max_len=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "PUJFw2EeYQLs"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Convert y_train to a list of tuples for hashability\n",
        "y_train_hashable = [tuple(row) for row in y_train]\n",
        "\n",
        "plt.hist(y_train_hashable, bins=len(set(y_train_hashable)))  # Plot histogram of labels\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Labels in Training Set')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "nR5poTlUV9fi",
        "outputId": "0c45e339-71aa-4cfa-8b05-2a99654172bd"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAJ8CAYAAAAS3luxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUiklEQVR4nO3deVxV1f7/8fcBBBQBRyZTcEpzNk2ytDRJnErtVmq3FDK7DZZGNtCgphZlqah5o0FFmxzKtJtlKmpdy/Q6NqmJOQs4i6CCwP790Y/z7cgg4MID8no+HvtRe+219/lsXMB5s/dex2ZZliUAAAAAwGVxcXYBAAAAAHA1IFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAajQxo4dK5vNdkVeq0uXLurSpYt9fc2aNbLZbPrss8+uyOtHREQoJCTkirxWSaWlpemhhx5SQECAbDabRo4ceUVeNyIiQlWrVjV6zIv/vUtq7969stlsio+Pv+xjXUk2m01jx44t0b4hISGKiIgwWg8AXAmEKwBXjfj4eNlsNvvi6empoKAghYeHa9q0aTpz5oyR1zl8+LDGjh2rrVu3GjmeSWW5tqJ47bXXFB8fr0cffVQffvihHnjggQL7hoSEqE+fPlewuvLv4u+RgpayHsJLU1pamsaMGaMWLVrIy8tLNWvWVJs2bTRixAgdPny42Mf7/fffNXbsWO3du9d8sQDKHDdnFwAApo0bN07169fXhQsXlJycrDVr1mjkyJGaPHmyvvzyS7Vq1cre96WXXtLzzz9frOMfPnxYr7zyikJCQtSmTZsi77d8+fJivU5JFFbb+++/r5ycnFKv4XKsWrVKN954o8aMGePsUsqM4OBgnTt3TpUqVbrsY91yyy368MMPHdoeeughdejQQQ8//LC9zcRVvHPnzsnNrWRvM3bu3CkXlyv/998LFy7olltu0Y4dOzRkyBA98cQTSktL02+//aZPPvlE/fv3V1BQULGO+fvvv+uVV15Rly5dKnRoBSoKwhWAq07Pnj3Vvn17+3p0dLRWrVqlPn366M4779T27dtVuXJlSZKbm1uJ3wAW1dmzZ1WlShW5u7uX6utciok356XtyJEjatasmbPLKFNyr8Ka0KBBAzVo0MCh7ZFHHlGDBg10//33F7hfVlaWcnJyijWGL6dmDw+PEu97ORYvXqwtW7bo448/1n333eew7fz588rMzHRKXQDKD24LBFAh3HbbbXr55Ze1b98+ffTRR/b2/J65WrFihTp16qRq1aqpatWqatKkiV544QVJfz0ndcMNN0iSIiMj7bdR5T4P06VLF7Vo0UKbNm3SLbfcoipVqtj3LegZnOzsbL3wwgsKCAiQl5eX7rzzTh04cMChT0HPoPz9mJeqLb9nrtLT0/X000+rbt268vDwUJMmTfTWW2/JsiyHfjabTcOHD9fixYvVokULeXh4qHnz5lq2bFn+X/CLHDlyREOHDpW/v788PT3VunVrzZkzx7499/mzPXv2aOnSpfbaL/dWqv/+97+65557VK9ePXl4eKhu3bp66qmndO7cuXz7//nnnwoPD5eXl5eCgoI0bty4PF+LnJwcxcbGqnnz5vL09JS/v7/+9a9/6eTJk5esZ/r06WrevLmqVKmi6tWrq3379vrkk08K3Se/Z65ynxE7dOiQ+vXrp6pVq6p27doaNWqUsrOzL/2FKcLrvfXWW4qNjVXDhg3l4eGh33//XZmZmRo9erTatWsnX19feXl5qXPnzlq9enWe41z8zFXu91piYqIiIiJUrVo1+fr6KjIyUmfPnnXY9+Lxnns74w8//KCoqCjVrl1bXl5e6t+/v44ePeqwb05OjsaOHaugoCBVqVJFXbt21e+//16k57h2794tSbr55pvzbPP09JSPj49D244dO3T33XerRo0a8vT0VPv27fXll1861H3PPfdIkrp27Wof12vWrCm0DgDlF1euAFQYDzzwgF544QUtX75cw4YNy7fPb7/9pj59+qhVq1YaN26cPDw8lJiYqB9++EGSdN1112ncuHEaPXq0Hn74YXXu3FmSdNNNN9mPcfz4cfXs2VMDBw7U/fffL39//0LrevXVV2Wz2fTcc8/pyJEjio2NVVhYmLZu3Wq/wlYURant7yzL0p133qnVq1dr6NChatOmjb799ls988wzOnTokKZMmeLQf+3atVq0aJEee+wxeXt7a9q0afrHP/6h/fv3q2bNmgXWde7cOXXp0kWJiYkaPny46tevr4ULFyoiIkKnTp3SiBEjdN111+nDDz/UU089pWuuuUZPP/20JKl27dpFPv/8LFy4UGfPntWjjz6qmjVrasOGDZo+fboOHjyohQsXOvTNzs5Wjx49dOONN2rixIlatmyZxowZo6ysLI0bN87e71//+pfi4+MVGRmpJ598Unv27NHbb7+tLVu26IcffijwCuH777+vJ598UnfffbdGjBih8+fP6+eff9b69evzXCUpiuzsbIWHhys0NFRvvfWWVq5cqUmTJqlhw4Z69NFHi328i82ePVvnz5/Xww8/LA8PD9WoUUOpqan64IMPNGjQIA0bNkxnzpzRzJkzFR4erg0bNhTpNtl7771X9evXV0xMjDZv3qwPPvhAfn5+euONNy657xNPPKHq1atrzJgx2rt3r2JjYzV8+HDNnz/f3ic6OloTJ07UHXfcofDwcG3btk3h4eE6f/78JY8fHBwsSZo7d65eeumlQie7+e2333TzzTerTp06ev755+Xl5aUFCxaoX79++vzzz9W/f3/dcsstevLJJzVt2jS98MILuu666yTJ/l8AVyELAK4Ss2fPtiRZ//vf/wrs4+vra7Vt29a+PmbMGOvvPwqnTJliSbKOHj1a4DH+97//WZKs2bNn59l26623WpKsuLi4fLfdeuut9vXVq1dbkqw6depYqamp9vYFCxZYkqypU6fa24KDg60hQ4Zc8piF1TZkyBArODjYvr548WJLkjVhwgSHfnfffbdls9msxMREe5sky93d3aFt27ZtliRr+vTpeV7r72JjYy1J1kcffWRvy8zMtDp27GhVrVrV4dyDg4Ot3r17F3q84vQ9e/ZsnraYmBjLZrNZ+/bts7cNGTLEkmQ98cQT9racnByrd+/elru7u308/Pe//7UkWR9//LHDMZctW5an/eJ/m759+1rNmzcv0rn93Z49e/L8m+bWO27cOIe+bdu2tdq1a1es43t5eTmMrdzX8/HxsY4cOeLQNysry8rIyHBoO3nypOXv7289+OCDDu2SrDFjxtjXc7/XLu7Xv39/q2bNmg5tF4/33O/tsLAwKycnx97+1FNPWa6urtapU6csy7Ks5ORky83NzerXr5/D8caOHWtJyvd76O/Onj1rNWnSxJJkBQcHWxEREdbMmTOtlJSUPH27detmtWzZ0jp//ry9LScnx7rpppusxo0b29sWLlxoSbJWr15d6GsDuDpwWyCACqVq1aqFzhpYrVo1SdKSJUtKPPmDh4eHIiMji9x/8ODB8vb2tq/ffffdCgwM1Ndff12i1y+qr7/+Wq6urnryyScd2p9++mlZlqVvvvnGoT0sLEwNGza0r7dq1Uo+Pj76888/L/k6AQEBGjRokL2tUqVKevLJJ5WWlqbvvvvOwNnk7+9X/tLT03Xs2DHddNNNsixLW7ZsydN/+PDh9v/PvRUyMzNTK1eulPTXlTBfX1/dfvvtOnbsmH1p166dqlatmu/tcbmqVaumgwcP6n//+5+x83vkkUcc1jt37nzJf4+i+sc//pHnyqGrq6v9uaucnBydOHFCWVlZat++vTZv3lzimo8fP67U1NRL7vvwww87XE3q3LmzsrOztW/fPklSQkKCsrKy9Nhjjzns98QTTxSptsqVK2v9+vV65plnJP11W9/QoUMVGBioJ554QhkZGZKkEydOaNWqVbr33nt15swZ+zg4fvy4wsPDtWvXLh06dKhIrwng6kK4AlChpKWlOQSZiw0YMEA333yzHnroIfn7+2vgwIFasGBBsYJWnTp1ivXgf+PGjR3WbTabGjVqVOpTN+/bt09BQUF5vh65tyzlvmHNVa9evTzHqF69+iWfNdq3b58aN26cZ/a3gl7HpP379ysiIkI1atSwP5d06623SpJOnz7t0NfFxSXPZA/XXnutJNn/LXbt2qXTp0/Lz89PtWvXdljS0tJ05MiRAmt57rnnVLVqVXXo0EGNGzfW448/br/dtCQ8PT3zhJ+i/HsUVf369fNtnzNnjlq1aiVPT0/VrFlTtWvX1tKlS/N8PQty8TiqXr26JBWp7kvtmzuWGjVq5NCvRo0a9r6X4uvrq4kTJ2rv3r3au3evZs6cqSZNmujtt9/W+PHjJUmJiYmyLEsvv/xynnGQO9NlYWMBwNWLZ64AVBgHDx7U6dOn87zx+rvKlSvr+++/1+rVq7V06VItW7ZM8+fP12233ably5fL1dX1kq9TnOekiqqgZz+ys7OLVJMJBb2OddGED2VFdna2br/9dp04cULPPfecmjZtKi8vLx06dEgRERElujKZk5MjPz8/ffzxx/luL+wZseuuu047d+7UV199pWXLlunzzz/Xv//9b40ePVqvvPJKsWsp7X/3/MbxRx99pIiICPXr10/PPPOM/Pz85OrqqpiYGPtkEJdyOePoSo/B4OBgPfjgg+rfv78aNGigjz/+WBMmTLCPnVGjRik8PDzffQv7OQPg6kW4AlBh5H6+T0FvhnK5uLioW7du6tatmyZPnqzXXntNL774olavXq2wsLBCH3IviV27djmsW5alxMREh8/jql69uk6dOpVn33379jlcbSlObcHBwVq5cqXOnDnjcPVqx44d9u0mBAcH6+eff1ZOTo7D1SvTr3OxX375RX/88YfmzJmjwYMH29tXrFiRb/+cnBz9+eef9qtVkvTHH39Ikn2WxYYNG2rlypW6+eabSxSivby8NGDAAA0YMECZmZm666679Oqrryo6OtrYdOul6bPPPlODBg20aNEih7FWVj6XLHcsJSYmOlx5O378+GVd0atevboaNmyoX3/9VZLs33OVKlVSWFhYofua/nkBoGzjtkAAFcKqVas0fvx41a9fX//85z8L7HfixIk8bbkzoOU+b+Hl5SVJ+Yadkpg7d67Dc2CfffaZkpKS1LNnT3tbw4YN9dNPPzl8zs5XX32VZ8r24tTWq1cvZWdn6+2333ZonzJlimw2m8PrX45evXopOTnZYUa3rKwsTZ8+XVWrVrXfpmda7lWOv1/VsCxLU6dOLXCfv38tLMvS22+/rUqVKqlbt26S/prpLjs723572N9lZWUV+nU/fvy4w7q7u7uaNWsmy7J04cKFIp2Ts+X3NV2/fr3WrVvnrJIcdOvWTW5ubnrnnXcc2i8e4wXZtm2bjh07lqd93759+v3339WkSRNJkp+fn7p06aJ3331XSUlJefr/fXp40z8vAJRtXLkCcNX55ptvtGPHDmVlZSklJUWrVq3SihUrFBwcrC+//LLQKwTjxo3T999/r969eys4OFhHjhzRv//9b11zzTXq1KmTpL+CTrVq1RQXFydvb295eXkpNDS0wGdULqVGjRrq1KmTIiMjlZKSotjYWDVq1MhhuviHHnpIn332mXr06KF7771Xu3fv1kcffeQwwURxa7vjjjvUtWtXvfjii9q7d69at26t5cuXa8mSJRo5cmSeY5fUww8/rHfffVcRERHatGmTQkJC9Nlnn+mHH35QbGxsoc/AXUpiYqImTJiQp71t27bq3r27GjZsqFGjRunQoUPy8fHR559/XuAVDE9PTy1btkxDhgxRaGiovvnmGy1dulQvvPCC/Xa/W2+9Vf/6178UExOjrVu3qnv37qpUqZJ27dqlhQsXaurUqbr77rvzPX737t0VEBCgm2++Wf7+/tq+fbvefvtt9e7d+7K+BldSnz59tGjRIvXv31+9e/fWnj17FBcXp2bNmiktLc3Z5cnf318jRozQpEmTdOedd6pHjx7atm2bvvnmG9WqVeuSV5FWrFihMWPG6M4779SNN96oqlWr6s8//9SsWbOUkZHh8LldM2bMUKdOndSyZUsNGzZMDRo0UEpKitatW6eDBw9q27Ztkv7644yrq6veeOMNnT59Wh4eHrrtttvk5+dXml8KAM7inEkKAcC83Omacxd3d3crICDAuv32262pU6c6TPmd6+Kp2BMSEqy+fftaQUFBlru7uxUUFGQNGjTI+uOPPxz2W7JkidWsWTPLzc3NYZrsW2+9tcDptguaiv3TTz+1oqOjLT8/P6ty5cpW7969HaYJzzVp0iSrTp06loeHh3XzzTdbGzduzHPMwmq7eCp2y7KsM2fOWE899ZQVFBRkVapUyWrcuLH15ptvOkx3bVl/Tav9+OOP56mpoCniL5aSkmJFRkZatWrVstzd3a2WLVvmO118cadi//u/99+XoUOHWpZlWb///rsVFhZmVa1a1apVq5Y1bNgw+xTyF09t7uXlZe3evdvq3r27VaVKFcvf398aM2aMlZ2dnee133vvPatdu3ZW5cqVLW9vb6tly5bWs88+ax0+fNje5+J/m3fffde65ZZbrJo1a1oeHh5Ww4YNrWeeecY6ffp0oedZ0FTsXl5eefpePJ6LoqCp2N988808fXNycqzXXnvNCg4Otjw8PKy2bdtaX331Vb5jSwVMxX7xxxzkft/u2bPH3lbQVOwXf8xC7vfQ36c5z8rKsl5++WUrICDAqly5snXbbbdZ27dvt2rWrGk98sgjhX4t/vzzT2v06NHWjTfeaPn5+Vlubm5W7dq1rd69e1urVq3K03/37t3W4MGDrYCAAKtSpUpWnTp1rD59+lifffaZQ7/333/fatCggeXq6sq07MBVzmZZZfRJZAAAAANOnTql6tWra8KECXrxxRedXQ6AqxjPXAEAgKvGuXPn8rTFxsZKkrp06XJliwFQ4fDMFQAAuGrMnz9f8fHx6tWrl6pWraq1a9fq008/Vffu3XXzzTc7uzwAVznCFQAAuGq0atVKbm5umjhxolJTU+2TXOQ38QkAmMYzVwAAAABgAM9cAQAAAIABhCsAAAAAMIBnrvKRk5Ojw4cPy9vb+5IfOAgAAADg6mVZls6cOaOgoCC5uBR+bYpwlY/Dhw+rbt26zi4DAAAAQBlx4MABXXPNNYX2IVzlw9vbW9JfX0AfHx8nVwMAAADAWVJTU1W3bl17RigM4SofubcC+vj4EK4AAAAAFOlxISa0AAAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAxwariKiYnRDTfcIG9vb/n5+alfv37auXPnJfdbuHChmjZtKk9PT7Vs2VJff/21w3bLsjR69GgFBgaqcuXKCgsL065du0rrNAAAAADAueHqu+++0+OPP66ffvpJK1as0IULF9S9e3elp6cXuM+PP/6oQYMGaejQodqyZYv69eunfv366ddff7X3mThxoqZNm6a4uDitX79eXl5eCg8P1/nz56/EaQEAAACogGyWZVnOLiLX0aNH5efnp++++0633HJLvn0GDBig9PR0ffXVV/a2G2+8UW3atFFcXJwsy1JQUJCefvppjRo1SpJ0+vRp+fv7Kz4+XgMHDrxkHampqfL19dXp06fl4+Nj5uQAAAAAlDvFyQZl6pmr06dPS5Jq1KhRYJ9169YpLCzMoS08PFzr1q2TJO3Zs0fJyckOfXx9fRUaGmrvc7GMjAylpqY6LAAAAABQHG7OLiBXTk6ORo4cqZtvvlktWrQosF9ycrL8/f0d2vz9/ZWcnGzfnttWUJ+LxcTE6JVXXrmc8ktVyPNLnV3CJe31vM/ZJRSqZf16zi6hUAtispxdQqGu27Hd2SUAAADDZjyyytklFOrxuNucXUKxlZkrV48//rh+/fVXzZs374q/dnR0tE6fPm1fDhw4cMVrAAAAAFC+lYkrV8OHD9dXX32l77//Xtdcc02hfQMCApSSkuLQlpKSooCAAPv23LbAwECHPm3atMn3mB4eHvLw8LiMMwAAAABQ0Tn1ypVlWRo+fLi++OILrVq1SvXr17/kPh07dlRCQoJD24oVK9SxY0dJUv369RUQEODQJzU1VevXr7f3AQAAAADTnHrl6vHHH9cnn3yiJUuWyNvb2/5MlK+vrypXrixJGjx4sOrUqaOYmBhJ0ogRI3Trrbdq0qRJ6t27t+bNm6eNGzfqvffekyTZbDaNHDlSEyZMUOPGjVW/fn29/PLLCgoKUr9+/ZxyngAAAACufk4NV++8844kqUuXLg7ts2fPVkREhCRp//79cnH5vwtsN910kz755BO99NJLeuGFF9S4cWMtXrzYYRKMZ599Vunp6Xr44Yd16tQpderUScuWLZOnp2epnxMAAACAismp4aooH7G1Zs2aPG333HOP7rnnngL3sdlsGjdunMaNG3c55QEAAABAkZWZ2QIBAAAAoDwjXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAp4ar77//XnfccYeCgoJks9m0ePHiQvtHRETIZrPlWZo3b27vM3bs2DzbmzZtWspnAgAAAKCic2q4Sk9PV+vWrTVjxowi9Z86daqSkpLsy4EDB1SjRg3dc889Dv2aN2/u0G/t2rWlUT4AAAAA2Lk588V79uypnj17Frm/r6+vfH197euLFy/WyZMnFRkZ6dDPzc1NAQEBxuoEAAAAgEsp189czZw5U2FhYQoODnZo37Vrl4KCgtSgQQP985//1P79+ws9TkZGhlJTUx0WAAAAACiOchuuDh8+rG+++UYPPfSQQ3toaKji4+O1bNkyvfPOO9qzZ486d+6sM2fOFHismJgY+1UxX19f1a1bt7TLBwAAAHCVKbfhas6cOapWrZr69evn0N6zZ0/dc889atWqlcLDw/X111/r1KlTWrBgQYHHio6O1unTp+3LgQMHSrl6AAAAAFcbpz5zVVKWZWnWrFl64IEH5O7uXmjfatWq6dprr1ViYmKBfTw8POTh4WG6TAAAAAAVSLm8cvXdd98pMTFRQ4cOvWTftLQ07d69W4GBgVegMgAAAAAVlVPDVVpamrZu3aqtW7dKkvbs2aOtW7faJ6CIjo7W4MGD8+w3c+ZMhYaGqkWLFnm2jRo1St9995327t2rH3/8Uf3795erq6sGDRpUqucCAAAAoGJz6m2BGzduVNeuXe3rUVFRkqQhQ4YoPj5eSUlJeWb6O336tD7//HNNnTo132MePHhQgwYN0vHjx1W7dm116tRJP/30k2rXrl16JwIAAACgwnNquOrSpYssyypwe3x8fJ42X19fnT17tsB95s2bZ6I0AAAAACiWcvnMFQAAAACUNYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGODUcPX999/rjjvuUFBQkGw2mxYvXlxo/zVr1shms+VZkpOTHfrNmDFDISEh8vT0VGhoqDZs2FCKZwEAAAAATg5X6enpat26tWbMmFGs/Xbu3KmkpCT74ufnZ982f/58RUVFacyYMdq8ebNat26t8PBwHTlyxHT5AAAAAGDn5swX79mzp3r27Fns/fz8/FStWrV8t02ePFnDhg1TZGSkJCkuLk5Lly7VrFmz9Pzzz19OuQAAAABQoHL5zFWbNm0UGBio22+/XT/88IO9PTMzU5s2bVJYWJi9zcXFRWFhYVq3bl2Bx8vIyFBqaqrDAgAAAADFUa7CVWBgoOLi4vT555/r888/V926ddWlSxdt3rxZknTs2DFlZ2fL39/fYT9/f/88z2X9XUxMjHx9fe1L3bp1S/U8AAAAAFx9nHpbYHE1adJETZo0sa/fdNNN2r17t6ZMmaIPP/ywxMeNjo5WVFSUfT01NZWABQAAAKBYylW4yk+HDh20du1aSVKtWrXk6uqqlJQUhz4pKSkKCAgo8BgeHh7y8PAo1ToBAAAAXN3K1W2B+dm6dasCAwMlSe7u7mrXrp0SEhLs23NycpSQkKCOHTs6q0QAAAAAFYBTr1ylpaUpMTHRvr5nzx5t3bpVNWrUUL169RQdHa1Dhw5p7ty5kqTY2FjVr19fzZs31/nz5/XBBx9o1apVWr58uf0YUVFRGjJkiNq3b68OHTooNjZW6enp9tkDAQAAAKA0ODVcbdy4UV27drWv5z73NGTIEMXHxyspKUn79++3b8/MzNTTTz+tQ4cOqUqVKmrVqpVWrlzpcIwBAwbo6NGjGj16tJKTk9WmTRstW7YszyQXAAAAAGCSzbIsy9lFlDWpqany9fXV6dOn5ePj4+xyFPL8UmeXcEl7Pe9zdgmFalm/nrNLKNSCmCxnl1Co63Zsd3YJAADAsBmPrHJ2CYV6PO42Z5cgqXjZoNw/cwUAAAAAZQHhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAY4NVx9//33uuOOOxQUFCSbzabFixcX2n/RokW6/fbbVbt2bfn4+Khjx4769ttvHfqMHTtWNpvNYWnatGkpngUAAAAAODlcpaenq3Xr1poxY0aR+n///fe6/fbb9fXXX2vTpk3q2rWr7rjjDm3ZssWhX/PmzZWUlGRf1q5dWxrlAwAAAICdmzNfvGfPnurZs2eR+8fGxjqsv/baa1qyZIn+85//qG3btvZ2Nzc3BQQEmCoTAAAAAC6pXD9zlZOTozNnzqhGjRoO7bt27VJQUJAaNGigf/7zn9q/f3+hx8nIyFBqaqrDAgAAAADFUa7D1VtvvaW0tDTde++99rbQ0FDFx8dr2bJleuedd7Rnzx517txZZ86cKfA4MTEx8vX1tS9169a9EuUDAAAAuIqU23D1ySef6JVXXtGCBQvk5+dnb+/Zs6fuuecetWrVSuHh4fr666916tQpLViwoMBjRUdH6/Tp0/blwIEDV+IUAAAAAFxFnPrMVUnNmzdPDz30kBYuXKiwsLBC+1arVk3XXnutEhMTC+zj4eEhDw8P02UCAAAAqEDK3ZWrTz/9VJGRkfr000/Vu3fvS/ZPS0vT7t27FRgYeAWqAwAAAFBROfXKVVpamsMVpT179mjr1q2qUaOG6tWrp+joaB06dEhz586V9NetgEOGDNHUqVMVGhqq5ORkSVLlypXl6+srSRo1apTuuOMOBQcH6/DhwxozZoxcXV01aNCgK3+CAAAAACoMp1652rhxo9q2bWufRj0qKkpt27bV6NGjJUlJSUkOM/299957ysrK0uOPP67AwED7MmLECHufgwcPatCgQWrSpInuvfde1axZUz/99JNq1659ZU8OAAAAQIXi1CtXXbp0kWVZBW6Pj493WF+zZs0ljzlv3rzLrAoAAAAAiq/cPXMFAAAAAGUR4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCgROHqzz//NF0HAAAAAJRrJQpXjRo1UteuXfXRRx/p/PnzpmsCAAAAgHKnROFq8+bNatWqlaKiohQQEKB//etf2rBhg+naAAAAAKDcKFG4atOmjaZOnarDhw9r1qxZSkpKUqdOndSiRQtNnjxZR48eNV0nAAAAAJRplzWhhZubm+666y4tXLhQb7zxhhITEzVq1CjVrVtXgwcPVlJSkqk6AQAAAKBMu6xwtXHjRj322GMKDAzU5MmTNWrUKO3evVsrVqzQ4cOH1bdvX1N1AgAAAECZ5laSnSZPnqzZs2dr586d6tWrl+bOnatevXrJxeWvrFa/fn3Fx8crJCTEZK0AAAAAUGaVKFy98847evDBBxUREaHAwMB8+/j5+WnmzJmXVRwAAAAAlBclCle7du26ZB93d3cNGTKkJIcHAAAAgHKnRM9czZ49WwsXLszTvnDhQs2ZM+eyiwIAAACA8qZE4SomJka1atXK0+7n56fXXnvtsosCAAAAgPKmROFq//79ql+/fp724OBg7d+//7KLAgAAAIDypkThys/PTz///HOe9m3btqlmzZqXXRQAAAAAlDclCleDBg3Sk08+qdWrVys7O1vZ2dlatWqVRowYoYEDB5quEQAAAADKvBLNFjh+/Hjt3btX3bp1k5vbX4fIycnR4MGDeeYKAAAAQIVUonDl7u6u+fPna/z48dq2bZsqV66sli1bKjg42HR9AAAAAFAulChc5br22mt17bXXmqoFAAAAAMqtEoWr7OxsxcfHKyEhQUeOHFFOTo7D9lWrVhkpDgAAAADKixKFqxEjRig+Pl69e/dWixYtZLPZTNcFAAAAAOVKicLVvHnztGDBAvXq1ct0PQAAAABQLpVoKnZ3d3c1atTIdC0AAAAAUG6VKFw9/fTTmjp1qizLMl0PAAAAAJRLJbotcO3atVq9erW++eYbNW/eXJUqVXLYvmjRIiPFAQAAAEB5UaJwVa1aNfXv3990LQAAAABQbpUoXM2ePdt0HQAAAABQrpXomStJysrK0sqVK/Xuu+/qzJkzkqTDhw8rLS3NWHEAAAAAUF6U6MrVvn371KNHD+3fv18ZGRm6/fbb5e3trTfeeEMZGRmKi4szXScAAAAAlGklunI1YsQItW/fXidPnlTlypXt7f3791dCQoKx4gAAAACgvCjRlav//ve/+vHHH+Xu7u7QHhISokOHDhkpDAAAAADKkxJducrJyVF2dnae9oMHD8rb2/uyiwIAAACA8qZE4ap79+6KjY21r9tsNqWlpWnMmDHq1auXqdoAAAAAoNwo0W2BkyZNUnh4uJo1a6bz58/rvvvu065du1SrVi19+umnpmsEAAAAgDKvROHqmmuu0bZt2zRv3jz9/PPPSktL09ChQ/XPf/7TYYILAAAAAKgoShSuJMnNzU3333+/yVoAAAAAoNwqUbiaO3duodsHDx5comIAAAAAoLwqUbgaMWKEw/qFCxd09uxZubu7q0qVKoQrAAAAABVOiWYLPHnypMOSlpamnTt3qlOnTkxoAQAAAKBCKlG4yk/jxo31+uuv57mqBQAAAAAVgbFwJf01ycXhw4dNHhIAAAAAyoUSPXP15ZdfOqxblqWkpCS9/fbbuvnmm40UBgAAAADlSYnCVb9+/RzWbTabateurdtuu02TJk0yURcAAAAAlCslui0wJyfHYcnOzlZycrI++eQTBQYGFvk433//ve644w4FBQXJZrNp8eLFl9xnzZo1uv766+Xh4aFGjRopPj4+T58ZM2YoJCREnp6eCg0N1YYNG4pxdgAAAABQfEafuSqu9PR0tW7dWjNmzChS/z179qh3797q2rWrtm7dqpEjR+qhhx7St99+a+8zf/58RUVFacyYMdq8ebNat26t8PBwHTlypLROAwAAAABKdltgVFRUkftOnjy5wG09e/ZUz549i3ysuLg41a9f337r4XXXXae1a9dqypQpCg8Pt7/esGHDFBkZad9n6dKlmjVrlp5//vkivxYAAAAAFEeJwtWWLVu0ZcsWXbhwQU2aNJEk/fHHH3J1ddX1119v72ez2cxU+f+tW7dOYWFhDm3h4eEaOXKkJCkzM1ObNm1SdHS0fbuLi4vCwsK0bt06o7UAAAAAwN+VKFzdcccd8vb21pw5c1S9enVJf32wcGRkpDp37qynn37aaJG5kpOT5e/v79Dm7++v1NRUnTt3TidPnlR2dna+fXbs2FHgcTMyMpSRkWFfT01NNVs4AAAAgKteicLVpEmTtHz5cnuwkqTq1atrwoQJ6t69e6mFq9ISExOjV155xdllAGXWjEdWObuES3o87jZnlwAAxRLy/FJnl1CovZ73ObuEQrWsX8/ZJRRqQUyWs0u4tC5Fm/cARVeiCS1SU1N19OjRPO1Hjx7VmTNnLruoggQEBCglJcWhLSUlRT4+PqpcubJq1aolV1fXfPsEBAQUeNzo6GidPn3avhw4cKBU6gcAAABw9SpRuOrfv78iIyO1aNEiHTx4UAcPHtTnn3+uoUOH6q677jJdo13Hjh2VkJDg0LZixQp17NhRkuTu7q527do59MnJyVFCQoK9T348PDzk4+PjsAAAAABAcZTotsC4uDiNGjVK9913ny5cuPDXgdzcNHToUL355ptFPk5aWpoSExPt63v27NHWrVtVo0YN1atXT9HR0Tp06JDmzp0rSXrkkUf09ttv69lnn9WDDz6oVatWacGCBVq69P8uq0dFRWnIkCFq3769OnTooNjYWKWnp9tnDwQAAACA0lCicFWlShX9+9//1ptvvqndu3dLkho2bCgvL69iHWfjxo3q2rWrfT13ivchQ4YoPj5eSUlJ2r9/v317/fr1tXTpUj311FOaOnWqrrnmGn3wwQf2adglacCAATp69KhGjx6t5ORktWnTRsuWLcszyQUAAAAAmFSicJUrKSlJSUlJuuWWW1S5cmVZllWs6de7dOkiy7IK3B4fH5/vPlu2bCn0uMOHD9fw4cOLXAcAAAAAXK4SPXN1/PhxdevWTddee6169eqlpKQkSdLQoUPL3UyBAAAAAGBCicLVU089pUqVKmn//v2qUqWKvX3AgAFatmyZseIAAAAAoLwo0W2By5cv17fffqtrrrnGob1x48bat2+fkcIAAAAAoDwp0ZWr9PR0hytWuU6cOCEPD4/LLgoAAAAAypsShavOnTvbp0eXJJvNppycHE2cONFh9j8AAAAAqChKdFvgxIkT1a1bN23cuFGZmZl69tln9dtvv+nEiRP64YcfTNcIAAAAAGVeia5ctWjRQn/88Yc6deqkvn37Kj09XXfddZe2bNmihg0bmq4RAAAAAMq8Yl+5unDhgnr06KG4uDi9+OKLpVETAAAAAJQ7xb5yValSJf3888+lUQsAAAAAlFslui3w/vvv18yZM03XAgAAAADlVokmtMjKytKsWbO0cuVKtWvXTl5eXg7bJ0+ebKQ4AAAAACgvihWu/vzzT4WEhOjXX3/V9ddfL0n6448/HPrYbDZz1QEAAABAOVGscNW4cWMlJSVp9erVkqQBAwZo2rRp8vf3L5XiAAAAAKC8KNYzV5ZlOax/8803Sk9PN1oQAAAAAJRHJZrQItfFYQsAAAAAKqpihSubzZbnmSqesQIAAACAYj5zZVmWIiIi5OHhIUk6f/68HnnkkTyzBS5atMhchQAAAABQDhQrXA0ZMsRh/f777zdaDAAAAACUV8UKV7Nnzy6tOgAAAACgXLusCS0AAAAAAH8hXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAFlIlzNmDFDISEh8vT0VGhoqDZs2FBg3y5dushms+VZevfube8TERGRZ3uPHj2uxKkAAAAAqKDcnF3A/PnzFRUVpbi4OIWGhio2Nlbh4eHauXOn/Pz88vRftGiRMjMz7evHjx9X69atdc899zj069Gjh2bPnm1f9/DwKL2TAAAAAFDhOf3K1eTJkzVs2DBFRkaqWbNmiouLU5UqVTRr1qx8+9eoUUMBAQH2ZcWKFapSpUqecOXh4eHQr3r16lfidAAAAABUUE4NV5mZmdq0aZPCwsLsbS4uLgoLC9O6deuKdIyZM2dq4MCB8vLycmhfs2aN/Pz81KRJEz366KM6fvy40doBAAAA4O+celvgsWPHlJ2dLX9/f4d2f39/7dix45L7b9iwQb/++qtmzpzp0N6jRw/dddddql+/vnbv3q0XXnhBPXv21Lp16+Tq6prnOBkZGcrIyLCvp6amlvCMAAAAAFRUTn/m6nLMnDlTLVu2VIcOHRzaBw4caP//li1bqlWrVmrYsKHWrFmjbt265TlOTEyMXnnllVKvFwAAAMDVy6m3BdaqVUuurq5KSUlxaE9JSVFAQECh+6anp2vevHkaOnToJV+nQYMGqlWrlhITE/PdHh0drdOnT9uXAwcOFP0kAAAAAEBODlfu7u5q166dEhIS7G05OTlKSEhQx44dC9134cKFysjI0P3333/J1zl48KCOHz+uwMDAfLd7eHjIx8fHYQEAAACA4nD6bIFRUVF6//33NWfOHG3fvl2PPvqo0tPTFRkZKUkaPHiwoqOj8+w3c+ZM9evXTzVr1nRoT0tL0zPPPKOffvpJe/fuVUJCgvr27atGjRopPDz8ipwTAAAAgIrH6c9cDRgwQEePHtXo0aOVnJysNm3aaNmyZfZJLvbv3y8XF8cMuHPnTq1du1bLly/PczxXV1f9/PPPmjNnjk6dOqWgoCB1795d48eP57OuAAAAAJQap4crSRo+fLiGDx+e77Y1a9bkaWvSpIksy8q3f+XKlfXtt9+aLA8AAAAALsnptwUCAAAAwNWAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADykS4mjFjhkJCQuTp6anQ0FBt2LChwL7x8fGy2WwOi6enp0Mfy7I0evRoBQYGqnLlygoLC9OuXbtK+zQAAAAAVGBOD1fz589XVFSUxowZo82bN6t169YKDw/XkSNHCtzHx8dHSUlJ9mXfvn0O2ydOnKhp06YpLi5O69evl5eXl8LDw3X+/PnSPh0AAAAAFZTTw9XkyZM1bNgwRUZGqlmzZoqLi1OVKlU0a9asAvex2WwKCAiwL/7+/vZtlmUpNjZWL730kvr27atWrVpp7ty5Onz4sBYvXnwFzggAAABAReTUcJWZmalNmzYpLCzM3ubi4qKwsDCtW7euwP3S0tIUHBysunXrqm/fvvrtt9/s2/bs2aPk5GSHY/r6+io0NLTQYwIAAADA5XBquDp27Jiys7MdrjxJkr+/v5KTk/Pdp0mTJpo1a5aWLFmijz76SDk5Obrpppt08OBBSbLvV5xjZmRkKDU11WEBAAAAgOJw+m2BxdWxY0cNHjxYbdq00a233qpFixapdu3aevfdd0t8zJiYGPn6+tqXunXrGqwYAAAAQEXg1HBVq1Ytubq6KiUlxaE9JSVFAQEBRTpGpUqV1LZtWyUmJkqSfb/iHDM6OlqnT5+2LwcOHCjuqQAAAACo4Jwartzd3dWuXTslJCTY23JycpSQkKCOHTsW6RjZ2dn65ZdfFBgYKEmqX7++AgICHI6Zmpqq9evXF3hMDw8P+fj4OCwAAAAAUBxuzi4gKipKQ4YMUfv27dWhQwfFxsYqPT1dkZGRkqTBgwerTp06iomJkSSNGzdON954oxo1aqRTp07pzTff1L59+/TQQw9J+msmwZEjR2rChAlq3Lix6tevr5dffllBQUHq16+fs04TAAAAwFXO6eFqwIABOnr0qEaPHq3k5GS1adNGy5Yts09IsX//frm4/N8FtpMnT2rYsGFKTk5W9erV1a5dO/34449q1qyZvc+zzz6r9PR0Pfzwwzp16pQ6deqkZcuW5fmwYQAAAAAwxenhSpKGDx+u4cOH57ttzZo1DutTpkzRlClTCj2ezWbTuHHjNG7cOFMlAgAAAEChyt1sgQAAAABQFhGuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMKBPhasaMGQoJCZGnp6dCQ0O1YcOGAvu+//776ty5s6pXr67q1asrLCwsT/+IiAjZbDaHpUePHqV9GgAAAAAqMKeHq/nz5ysqKkpjxozR5s2b1bp1a4WHh+vIkSP59l+zZo0GDRqk1atXa926dapbt666d++uQ4cOOfTr0aOHkpKS7Munn356JU4HAAAAQAXl9HA1efJkDRs2TJGRkWrWrJni4uJUpUoVzZo1K9/+H3/8sR577DG1adNGTZs21QcffKCcnBwlJCQ49PPw8FBAQIB9qV69+pU4HQAAAAAVlFPDVWZmpjZt2qSwsDB7m4uLi8LCwrRu3boiHePs2bO6cOGCatSo4dC+Zs0a+fn5qUmTJnr00Ud1/PjxAo+RkZGh1NRUhwUAAAAAisOp4erYsWPKzs6Wv7+/Q7u/v7+Sk5OLdIznnntOQUFBDgGtR48emjt3rhISEvTGG2/ou+++U8+ePZWdnZ3vMWJiYuTr62tf6tatW/KTAgAAAFAhuTm7gMvx+uuva968eVqzZo08PT3t7QMHDrT/f8uWLdWqVSs1bNhQa9asUbdu3fIcJzo6WlFRUfb11NRUAhYAAACAYnHqlatatWrJ1dVVKSkpDu0pKSkKCAgodN+33npLr7/+upYvX65WrVoV2rdBgwaqVauWEhMT893u4eEhHx8fhwUAAAAAisOp4crd3V3t2rVzmIwid3KKjh07FrjfxIkTNX78eC1btkzt27e/5OscPHhQx48fV2BgoJG6AQAAAOBiTp8tMCoqSu+//77mzJmj7du369FHH1V6eroiIyMlSYMHD1Z0dLS9/xtvvKGXX35Zs2bNUkhIiJKTk5WcnKy0tDRJUlpamp555hn99NNP2rt3rxISEtS3b181atRI4eHhTjlHAAAAAFc/pz9zNWDAAB09elSjR49WcnKy2rRpo2XLltknudi/f79cXP4vA77zzjvKzMzU3Xff7XCcMWPGaOzYsXJ1ddXPP/+sOXPm6NSpUwoKClL37t01fvx4eXh4XNFzAwAAAFBxOD1cSdLw4cM1fPjwfLetWbPGYX3v3r2FHqty5cr69ttvDVUGAAAAAEXj9NsCAQAAAOBqQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGEC4AgAAAAADCFcAAAAAYADhCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAAAAABhAuAIAAAAAAwhXAAAAAGAA4QoAAAAADCBcAQAAAIABhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAWUiXM2YMUMhISHy9PRUaGioNmzYUGj/hQsXqmnTpvL09FTLli319ddfO2y3LEujR49WYGCgKleurLCwMO3atas0TwEAAABABef0cDV//nxFRUVpzJgx2rx5s1q3bq3w8HAdOXIk3/4//vijBg0apKFDh2rLli3q16+f+vXrp19//dXeZ+LEiZo2bZri4uK0fv16eXl5KTw8XOfPn79SpwUAAACggnF6uJo8ebKGDRumyMhINWvWTHFxcapSpYpmzZqVb/+pU6eqR48eeuaZZ3Tddddp/Pjxuv766/X2229L+uuqVWxsrF566SX17dtXrVq10ty5c3X48GEtXrz4Cp4ZAAAAgIrEzZkvnpmZqU2bNik6Otre5uLiorCwMK1bty7ffdatW6eoqCiHtvDwcHtw2rNnj5KTkxUWFmbf7uvrq9DQUK1bt04DBw7Mc8yMjAxlZGTY10+fPi1JSk1NLfG5mZSTcdbZJVxSqs1ydgmFyj6X7ewSCpWWXbbrO5eZ7uwSLqmsfL8CQFGV9d/v/G6/PGX9d7tU9n+/l5Xf7bl1WNalvyecGq6OHTum7Oxs+fv7O7T7+/trx44d+e6TnJycb//k5GT79ty2gvpcLCYmRq+88kqe9rp16xbtRCBfZxdwSdudXUChOji7gEtJvNPZFVzSM7OdXQEAXF343X55yvzvdqnM/34va7/bz5w5I1/fwr8znBquyoro6GiHq2E5OTk6ceKEatasKZvNVuB+qampqlu3rg4cOCAfH58rUSoqCMYWSgtjC6WFsYXSwthCaSnq2LIsS2fOnFFQUNAlj+nUcFWrVi25uroqJSXFoT0lJUUBAQH57hMQEFBo/9z/pqSkKDAw0KFPmzZt8j2mh4eHPDw8HNqqVatW5PPw8fHhmx2lgrGF0sLYQmlhbKG0MLZQWooyti51xSqXUye0cHd3V7t27ZSQkGBvy8nJUUJCgjp27JjvPh07dnToL0krVqyw969fv74CAgIc+qSmpmr9+vUFHhMAAAAALpfTbwuMiorSkCFD1L59e3Xo0EGxsbFKT09XZGSkJGnw4MGqU6eOYmJiJEkjRozQrbfeqkmTJql3796aN2+eNm7cqPfee0+SZLPZNHLkSE2YMEGNGzdW/fr19fLLLysoKEj9+vVz1mkCAAAAuMo5PVwNGDBAR48e1ejRo5WcnKw2bdpo2bJl9gkp9u/fLxeX/7vAdtNNN+mTTz7RSy+9pBdeeEGNGzfW4sWL1aJFC3ufZ599Vunp6Xr44Yd16tQpderUScuWLZOnp6fR2j08PDRmzJg8txQCl4uxhdLC2EJpYWyhtDC2UFpKY2zZrKLMKQgAAAAAKJTTP0QYAAAAAK4GhCsAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLi6BCZTBFDWZWRk2P+fn1kw6ciRI9q9e7ezy8BV6OKfVTk5OU6qBDCLcPU3Bw8e1LfffquFCxdq3759kv76UGK+4XG5UlJStGnTJq1YsUJnz551djm4ivz+++/6xz/+oYSEBEl//cwiYMGEn3/+WZ07d9a3336ro0ePOrscXEV27dqlZ599Vo899pgmTpwoSQ6faQpcjuzsbKe+vtM/RLis+OWXX3T77berXr162rx5s9q2bauOHTtq2rRpcnFxUU5ODt/4KJFffvlFAwYMkLu7u37++Wf17NlTb7zxhsMHXwMlYVmWJk6cqLVr18pms0mSunXrZg9YuW1Ace3atUu33Xab7r//fg0ePFhVq1Z12M7vRJTUL7/8ottuu01du3bV8ePH9dNPP8nHx0ePPPKIJPGzC5dl+/btmj59unbv3q2bbrpJHTt2VPfu3a9oDfxklHT69Gk98MADGjRokFasWKF9+/apb9++Wr16tfr06SNJ9oAFFMeuXbsUHh6uf/zjH/riiy+0fft2/fzzz5o5c6azS8NVwGazycvLS02bNlWlSpX0+uuva8WKFfZtQEm9++676t69u2JjY+Xl5aV58+Zp+vTp+vDDDyXxOxElc+zYMd1///168MEHtWDBAi1atEgBAQE6d+6cvQ93DKGkduzYoY4dO+rMmTOqWbOm1q5dq/vuu0+xsbFXtA6uXOmvcHXu3Dnde++98vX1la+vr0aOHKkmTZro5Zdf1r333qsFCxbwVzoUy7lz5zRp0iT16tVLL7/8slxdXeXq6qqXXnpJ06dPV0ZGhtzd3XkTjMvSqVMn1atXT127dtXo0aP11ltvqXbt2lq+fLkGDhyoevXqObtElEP79u1T586dJUk33XSTKlWqpMOHD0uSZsyYoR9//FEuLi5cZUCx7N+/X5mZmXr44YclSb6+vgoICNDatWu1ceNG+fr66t///jd3DKFE3nvvPd122232PwLt379fn3zyiaKiopSRkaHnnnvuitTBqJXk7e2tCxcu6Mcff7S3Va1aVXfeeadeeOEF7dy5U++++64TK0R5lJ2drczMTHXq1Enu7u5ydXWVJAUEBOjEiRPKzMx0coW4Gnh7e+vLL79Uhw4d9Mwzz8jLy0t9+vTR888/Lw8PD0lMcoHiy8rK0tatWxUXFycfHx998cUXWr9+vT7++GOlpqaqX79+krhCiuLx8vLS2bNn9dFHHykrK0vjx4/Xhx9+qMaNG8vPz0+rVq2yh3qCFYrDsizt3btX7u7u9rZ69erpiSee0KRJk/Tyyy9r9uzZV6QWRq6kKlWq6JZbbtHKlSv1yy+/2Ns9PDx09913KyQkRGvWrHFegSiXqlatqldffVURERGS/u8By4CAANWsWVNVq1a1vzHZsWOHs8pEOXfttdfax1a3bt105swZnTx5UqGhodq1a5ck3gCj6HJvx+rfv78OHDigRYsW6cYbb1TNmjVVs2ZNhYaGasyYMdq9e7f27Nnj5GpR3gQGBmrgwIF6//331atXL40bN07z58/X66+/rilTpuidd95RYmKivvvuO2eXinLGZrPplltu0bZt27R9+3Z7u5eXlyIiIvT444/r/ffft1+BL02EK/0VokaNGqUtW7ZowoQJDtPOVqlSRbfeeqv++OMPZnlDsQUGBkr66w1L7pWrnJwcpaam2sfTiy++qBEjRuj06dNOqxPlV6NGjeTh4aEDBw5o8ODB+v333/XWW28pICBAUVFR+v77751dIsqR3KsFXbp00YULF7Ry5co8ISowMFDZ2dlcWUCx+fj46KWXXtJ///tfvfTSS2ratKluueUWh+1Vq1aVt7e3E6tEedW+fXt5e3srPj5eBw8etLdXr15dvXv31q+//qqkpKRSr4NnrvTXm90WLVpoyZIl6tatm3JycvTYY4+pa9eukv66qnDNNdfIzY0vF0rm729CMjMzdebMGbm5uWnMmDGaOHGi1q1bJ19fXydWiPLIsixlZWXJsix17NhRLi4uWrp0qdq0aaPg4GDNnTtXISEhzi4T5YxlWapXr57ee+89DRw4UEuXLlVMTIyio6OVkZGhhIQE1axZUz4+Ps4uFeWQt7e3vL29lZOTIw8PD23fvt1+K+CSJUtUtWpV1alTx8lVojzq1KmTBg0apKlTp8rDw0MRERFq0KCBJKlly5aqV6+ew+dClhabVYFuxs/JyZFlWfYrCLltLi4uys7OlqurqzZt2qSHHnrI3hYSEqLVq1fr+++/V+vWrZ1YPcqywsbWxX766Sc9+eSTuvXWWzV9+nT98MMPateu3ZUsF+VIUcbWxx9/rOnTp2vGjBkOYyk9PV1eXl5XtF6UH4WNrdz//vHHH3rxxRe1fv16ZWVlqXHjxvr111+VkJCgNm3aOK94lGlF+bl15MgR9e7dW9WrV1f16tVVtWpVffHFF1q1ahVjC8X29/H12muvae7cuWrXrp0iIiLUqFEjvfPOO/r000/1v//9TwEBAaVaS4UJV7///rtee+01JScnq3HjxurTp4969+4tSfZglfvf/fv3a9OmTVq1apXq1q2rO++8U02bNnXyGaCsKsrY+rsff/xRnTp1UvXq1bVixQpdf/31zigb5UBRx9aFCxeUnp6uatWqSeJzYnBpRRlbuW9Wjh8/roMHD+qbb75RvXr1FBoaqoYNGzr5DFBWFWVs5f6M2r59u6ZNm6a9e/cqODhYI0aM0HXXXefkM0BZlt/7qlx/D1hz5szR4sWL9eWXX6p58+ZKTU3VF198obZt25Z6jRUiXO3cuVOhoaHq2bOnQkJC9M0336hSpUrq1KmTpkyZIumvW7Xc3d15U4JiKc7YyrV3717de++9io+PV7NmzZxVOsq4ooytjIwM+4yAEh/siqIpyc8toCiKM7Zyf16dO3dOlStX1oULF1SpUiUnnwHKsj/++EP/+c9/dN9999mfab9YVlaW/TGe9PR07dmzRy4uLqpZs6b8/f2vSJ1XfbiyLEsvvfSSEhMTNX/+fEnSmTNnNG3aNH322We64YYb9N5779n7L1myRB07dpSfn5+zSkY5UdyxlTtddkBAQJ43xcDflWRsdezYUbVr13ZWySgnGFsoLZf7fos/bqMwiYmJCg0N1cmTJ/X8888rKipKtWrVcuhTVsbQVf8nTpvNpsOHDys5Odne5u3trSeffFL333+/tmzZotdff12StHTpUg0fPlzTpk3j08FxScUdW48//rimT5+u7Oxs/iKMQpVkbE2dOpWfW7gkxhZKy+W+3yoLb4pRNqWnpysmJkZ33nmn3n77bb3++uuaOHGijh075tAvdwy9+eabGj9+vDNKlXSVh6vci3LXX3+9srOztXPnTvs2b29vPfjgg2rbtq3+85//KDMzU71799aDDz6oBx98kFtrUKiSjq2hQ4fK1dWVXyIoED+3UFoYWygtjC2UJhcXF7Vr1049evTQY489pnnz5umtt97KN2CdOHFCmzZt0tKlS3XixAnnFGxVAImJiVatWrWsBx980Dpz5oxlWZaVk5NjWZZl7d+/37LZbNZ//vMfZ5aIcoqxhdLC2EJpYWyhtDC2UFrS0tIc1ufNm2fZbDZr1KhR1rFjxyzLsqysrCzr5MmT1vHjx63Dhw87o0zLsiyrQnxwU8OGDbVgwQL17NlTlStX1tixY+33aVaqVEmtWrVSzZo1nVwlyiPGFkoLYwulhbGF0sLYQmnJ/ViR3A8wHzBggCzL0n333SebzaaRI0fqzTff1N69ezVv3jzVqFHDabVWiHAlSV27dtXChQt1zz33KCkpSffee69atWqluXPn6siRI6pbt66zS0Q5xdhCaWFsobQwtlBaGFsoTblT+efk5GjgwIGy2Wx64IEH9OWXX2r37t3asGGD0ycMu+pnC7zY5s2bFRUVpb1798rNzU2urq6aN2/eFZn3Hlc3xhZKC2MLpYWxhdLC2EJpyo0vNptN3bp109atW7VmzRq1bNnSyZVVwHAlSampqTpx4oTOnDmjwMDAPFM5AiXF2EJpYWyhtDC2UFoYWyhN2dnZeuaZZxQbG6utW7eqVatWzi5JUgUNVwAAAADKr+zsbMXHx6tdu3Zq06aNs8uxI1wBAAAAKHesMvLBwX/HhwsAAAAAKHfKWrCSCFcAAAAAYAThCgAAAAAMIFwBAAAAgAGEKwAAAAAwgHAFAAAAAAYQrgAAAADAAMIVAACS4uPjVa1atcs+js1m0+LFiy/7OACA8odwBQC4akRERKhfv37OLgMAUEERrgAAAADAAMIVAKBCmDx5slq2bCkvLy/VrVtXjz32mNLS0vL0W7x4sRo3bixPT0+Fh4frwIEDDtuXLFmi66+/Xp6enmrQoIFeeeUVZWVl5fuamZmZGj58uAIDA+Xp6ang4GDFxMSUyvkBAJyPcAUAqBBcXFw0bdo0/fbbb5ozZ45WrVqlZ5991qHP2bNn9eqrr2ru3Ln64YcfdOrUKQ0cONC+/b///a8GDx6sESNG6Pfff9e7776r+Ph4vfrqq/m+5rRp0/Tll19qwYIF2rlzpz7++GOFhISU5mkCAJzIZlmW5ewiAAAwISIiQqdOnSrShBKfffaZHnnkER07dkzSXxNaREZG6qefflJoaKgkaceOHbruuuu0fv16dejQQWFhYerWrZuio6Ptx/noo4/07LPP6vDhw5L+mtDiiy++UL9+/fTkk0/qt99+08qVK2Wz2cyfMACgTOHKFQCgQli5cqW6deumOnXqyNvbWw888ICOHz+us2fP2vu4ubnphhtusK83bdpU1apV0/bt2yVJ27Zt07hx41S1alX7MmzYMCUlJTkcJ1dERIS2bt2qJk2a6Mknn9Ty5ctL/0QBAE5DuAIAXPX27t2rPn36qFWrVvr888+1adMmzZgxQ9Jfz0UVVVpaml555RVt3brVvvzyyy/atWuXPD098/S//vrrtWfPHo0fP17nzp3Tvffeq7vvvtvYeQEAyhY3ZxcAAEBp27Rpk3JycjRp0iS5uPz1d8UFCxbk6ZeVlaWNGzeqQ4cOkqSdO3fq1KlTuu666yT9FZZ27typRo0aFfm1fXx8NGDAAA0YMEB33323evTooRMnTqhGjRoGzgwAUJYQrgAAV5XTp09r69atDm21atXShQsXNH36dN1xxx364YcfFBcXl2ffSpUq6YknntC0adPk5uam4cOH68Ybb7SHrdGjR6tPnz6qV6+e7r77brm4uGjbtm369ddfNWHChDzHmzx5sgIDA9W2bVu5uLho4cKFCggIMPJhxQCAsofbAgEAV5U1a9aobdu2DsuHH36oyZMn64033lCLFi308ccf5zslepUqVfTcc8/pvvvu080336yqVatq/vz59u3h4eH66quvtHz5ct1www268cYbNWXKFAUHB+dbi7e3tyZOnKj27dvrhhtu0N69e/X111/br54BAK4uzBYIAAAAAAbwpzMAAAAAMIBwBQAAAAAGEK4AAAAAwADCFQAAAAAYQLgCAAAAAAMIVwAAAABgAOEKAAAAAAwgXAEAAACAAYQrAAAAADCAcAUAAAAABhCuAAAAAMAAwhUAAAAAGPD/AMDTu5h7qlWsAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping Parameters\n",
        "best_val_loss = float('inf')  # Initialize with a very large value\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "epochs_without_improvement = 0"
      ],
      "metadata": {
        "id": "Uh0bDkYm4Fko"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "model = TreeClassifier(num_classes=3, dropout_rate=dropout_rate).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "start = timer()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        # Retrieve data from batch\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass: Get predictions from the model\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Shape: [batch_size, num_classes]\n",
        "\n",
        "        # Collect predictions and true labels\n",
        "        # Convert model outputs to multi-label predictions using a threshold\n",
        "        threshold = 0.5  # Adjust as needed\n",
        "        preds = (outputs > threshold).cpu().numpy()  # Apply threshold to get multi-label predictions\n",
        "        all_preds.extend(preds)  # Store all predictions\n",
        "        all_labels.extend(labels.cpu().numpy())  # Store all true labels\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels.float())\n",
        "\n",
        "        # Backward pass: Optimize the model\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate overall accuracy for the epoch\n",
        "    #epoch_accuracy = compute_accuracy(all_labels, all_preds)\n",
        "    epoch_hamming_loss = hamming_loss(all_labels, all_preds)\n",
        "\n",
        "    # Logging the loss and accuracy for this epoch\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    if (epoch + 1) % 1 == 0:  # print every epoch (adjust the '1' to control frequency)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Hamming Loss: {epoch_hamming_loss:.4f}\")\n",
        "\n",
        "    # Validation Phase (Calculate val_loss)\n",
        "    model.eval()\n",
        "    val_loss = 0  # Initialize validation loss\n",
        "    with torch.no_grad():\n",
        "        for val_batch in data_loader:\n",
        "            input_ids = val_batch['input_ids'].to(device)\n",
        "            attention_mask = val_batch['attention_mask'].to(device)\n",
        "            labels = val_batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(data_loader)  # Average validation loss\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")  # Print validation loss\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        # Save the model's state_dict (optional)\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n",
        "end = timer()\n",
        "print_train_time(start, end, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OApsIdn3W8RI",
        "outputId": "ae570208-44d1-4915-de43-dc30092388ba"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.7731, Hamming Loss: 0.4667\n",
            "Validation Loss: 0.7634\n",
            "Epoch 2/200, Loss: 0.7639, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7577\n",
            "Epoch 3/200, Loss: 0.7592, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7553\n",
            "Epoch 4/200, Loss: 0.7538, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7523\n",
            "Epoch 5/200, Loss: 0.7537, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7483\n",
            "Epoch 6/200, Loss: 0.7512, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7436\n",
            "Epoch 7/200, Loss: 0.7453, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7378\n",
            "Epoch 8/200, Loss: 0.7389, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7309\n",
            "Epoch 9/200, Loss: 0.7339, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7224\n",
            "Epoch 10/200, Loss: 0.7264, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7141\n",
            "Epoch 11/200, Loss: 0.7222, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.7044\n",
            "Epoch 12/200, Loss: 0.7097, Hamming Loss: 0.2667\n",
            "Validation Loss: 0.6980\n",
            "Epoch 13/200, Loss: 0.7133, Hamming Loss: 0.3333\n",
            "Validation Loss: 0.6951\n",
            "Epoch 14/200, Loss: 0.7076, Hamming Loss: 0.2000\n",
            "Validation Loss: 0.6954\n",
            "Epoch 15/200, Loss: 0.7167, Hamming Loss: 0.2000\n",
            "Validation Loss: 0.6943\n",
            "Epoch 16/200, Loss: 0.7055, Hamming Loss: 0.1333\n",
            "Validation Loss: 0.6904\n",
            "Epoch 17/200, Loss: 0.6911, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6840\n",
            "Epoch 18/200, Loss: 0.6846, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6768\n",
            "Epoch 19/200, Loss: 0.6815, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6696\n",
            "Epoch 20/200, Loss: 0.6762, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6631\n",
            "Epoch 21/200, Loss: 0.6734, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6573\n",
            "Epoch 22/200, Loss: 0.6645, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6553\n",
            "Epoch 23/200, Loss: 0.6717, Hamming Loss: 0.1333\n",
            "Validation Loss: 0.6521\n",
            "Epoch 24/200, Loss: 0.6692, Hamming Loss: 0.1333\n",
            "Validation Loss: 0.6494\n",
            "Epoch 25/200, Loss: 0.6635, Hamming Loss: 0.1333\n",
            "Validation Loss: 0.6488\n",
            "Epoch 26/200, Loss: 0.6527, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6491\n",
            "Epoch 27/200, Loss: 0.6578, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6478\n",
            "Epoch 28/200, Loss: 0.6503, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6448\n",
            "Epoch 29/200, Loss: 0.6475, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6411\n",
            "Epoch 30/200, Loss: 0.6419, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6373\n",
            "Epoch 31/200, Loss: 0.6412, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6330\n",
            "Epoch 32/200, Loss: 0.6395, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6287\n",
            "Epoch 33/200, Loss: 0.6336, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6246\n",
            "Epoch 34/200, Loss: 0.6294, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6215\n",
            "Epoch 35/200, Loss: 0.6270, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6187\n",
            "Epoch 36/200, Loss: 0.6266, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6164\n",
            "Epoch 37/200, Loss: 0.6220, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6144\n",
            "Epoch 38/200, Loss: 0.6186, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6126\n",
            "Epoch 39/200, Loss: 0.6200, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6109\n",
            "Epoch 40/200, Loss: 0.6196, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6100\n",
            "Epoch 41/200, Loss: 0.6144, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6097\n",
            "Epoch 42/200, Loss: 0.6154, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6091\n",
            "Epoch 43/200, Loss: 0.6105, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6081\n",
            "Epoch 44/200, Loss: 0.6133, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6065\n",
            "Epoch 45/200, Loss: 0.6092, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6045\n",
            "Epoch 46/200, Loss: 0.6102, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.6019\n",
            "Epoch 47/200, Loss: 0.6067, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5994\n",
            "Epoch 48/200, Loss: 0.6051, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5971\n",
            "Epoch 49/200, Loss: 0.6018, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5951\n",
            "Epoch 50/200, Loss: 0.5997, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5935\n",
            "Epoch 51/200, Loss: 0.6009, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5920\n",
            "Epoch 52/200, Loss: 0.5994, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5908\n",
            "Epoch 53/200, Loss: 0.5968, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5896\n",
            "Epoch 54/200, Loss: 0.5969, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5886\n",
            "Epoch 55/200, Loss: 0.5932, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5877\n",
            "Epoch 56/200, Loss: 0.5926, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5871\n",
            "Epoch 57/200, Loss: 0.5917, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5865\n",
            "Epoch 58/200, Loss: 0.5908, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5857\n",
            "Epoch 59/200, Loss: 0.5894, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5848\n",
            "Epoch 60/200, Loss: 0.5889, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5837\n",
            "Epoch 61/200, Loss: 0.5902, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5826\n",
            "Epoch 62/200, Loss: 0.5877, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5815\n",
            "Epoch 63/200, Loss: 0.5859, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5805\n",
            "Epoch 64/200, Loss: 0.5846, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5798\n",
            "Epoch 65/200, Loss: 0.5846, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5792\n",
            "Epoch 66/200, Loss: 0.5842, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5786\n",
            "Epoch 67/200, Loss: 0.5821, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5780\n",
            "Epoch 68/200, Loss: 0.5820, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5773\n",
            "Epoch 69/200, Loss: 0.5827, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5767\n",
            "Epoch 70/200, Loss: 0.5812, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5760\n",
            "Epoch 71/200, Loss: 0.5796, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5754\n",
            "Epoch 72/200, Loss: 0.5808, Hamming Loss: 0.0667\n",
            "Validation Loss: 0.5749\n",
            "Epoch 73/200, Loss: 0.5786, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5744\n",
            "Epoch 74/200, Loss: 0.5781, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5740\n",
            "Epoch 75/200, Loss: 0.5775, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5736\n",
            "Epoch 76/200, Loss: 0.5763, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5731\n",
            "Epoch 77/200, Loss: 0.5762, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5725\n",
            "Epoch 78/200, Loss: 0.5751, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5719\n",
            "Epoch 79/200, Loss: 0.5745, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5711\n",
            "Epoch 80/200, Loss: 0.5740, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5704\n",
            "Epoch 81/200, Loss: 0.5728, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5697\n",
            "Epoch 82/200, Loss: 0.5722, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5690\n",
            "Epoch 83/200, Loss: 0.5727, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5683\n",
            "Epoch 84/200, Loss: 0.5719, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5676\n",
            "Epoch 85/200, Loss: 0.5708, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5668\n",
            "Epoch 86/200, Loss: 0.5699, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5661\n",
            "Epoch 87/200, Loss: 0.5697, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5655\n",
            "Epoch 88/200, Loss: 0.5687, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5650\n",
            "Epoch 89/200, Loss: 0.5703, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5645\n",
            "Epoch 90/200, Loss: 0.5677, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5640\n",
            "Epoch 91/200, Loss: 0.5677, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5634\n",
            "Epoch 92/200, Loss: 0.5679, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5629\n",
            "Epoch 93/200, Loss: 0.5664, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5623\n",
            "Epoch 94/200, Loss: 0.5678, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5618\n",
            "Epoch 95/200, Loss: 0.5661, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5614\n",
            "Epoch 96/200, Loss: 0.5651, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5610\n",
            "Epoch 97/200, Loss: 0.5655, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5607\n",
            "Epoch 98/200, Loss: 0.5642, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5605\n",
            "Epoch 99/200, Loss: 0.5635, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5602\n",
            "Epoch 100/200, Loss: 0.5628, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5600\n",
            "Epoch 101/200, Loss: 0.5628, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5597\n",
            "Epoch 102/200, Loss: 0.5617, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5595\n",
            "Epoch 103/200, Loss: 0.5610, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5592\n",
            "Epoch 104/200, Loss: 0.5609, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5589\n",
            "Epoch 105/200, Loss: 0.5615, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5583\n",
            "Epoch 106/200, Loss: 0.5597, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5578\n",
            "Epoch 107/200, Loss: 0.5607, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5573\n",
            "Epoch 108/200, Loss: 0.5602, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5566\n",
            "Epoch 109/200, Loss: 0.5596, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5560\n",
            "Epoch 110/200, Loss: 0.5583, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5555\n",
            "Epoch 111/200, Loss: 0.5585, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5552\n",
            "Epoch 112/200, Loss: 0.5580, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5550\n",
            "Epoch 113/200, Loss: 0.5580, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5548\n",
            "Epoch 114/200, Loss: 0.5569, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5547\n",
            "Epoch 115/200, Loss: 0.5568, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5544\n",
            "Epoch 116/200, Loss: 0.5563, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5542\n",
            "Epoch 117/200, Loss: 0.5568, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5539\n",
            "Epoch 118/200, Loss: 0.5556, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5537\n",
            "Epoch 119/200, Loss: 0.5559, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5534\n",
            "Epoch 120/200, Loss: 0.5555, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5531\n",
            "Epoch 121/200, Loss: 0.5553, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5528\n",
            "Epoch 122/200, Loss: 0.5546, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5525\n",
            "Epoch 123/200, Loss: 0.5550, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5523\n",
            "Epoch 124/200, Loss: 0.5549, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5521\n",
            "Epoch 125/200, Loss: 0.5544, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5518\n",
            "Epoch 126/200, Loss: 0.5541, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5516\n",
            "Epoch 127/200, Loss: 0.5542, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5514\n",
            "Epoch 128/200, Loss: 0.5537, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5512\n",
            "Epoch 129/200, Loss: 0.5532, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5511\n",
            "Epoch 130/200, Loss: 0.5536, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5509\n",
            "Epoch 131/200, Loss: 0.5538, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5509\n",
            "Epoch 132/200, Loss: 0.5524, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5508\n",
            "Epoch 133/200, Loss: 0.5522, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5507\n",
            "Epoch 134/200, Loss: 0.5525, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5505\n",
            "Epoch 135/200, Loss: 0.5527, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5503\n",
            "Epoch 136/200, Loss: 0.5519, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5501\n",
            "Epoch 137/200, Loss: 0.5518, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5500\n",
            "Epoch 138/200, Loss: 0.5522, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5500\n",
            "Epoch 139/200, Loss: 0.5516, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5499\n",
            "Epoch 140/200, Loss: 0.5519, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5498\n",
            "Epoch 141/200, Loss: 0.5516, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5497\n",
            "Epoch 142/200, Loss: 0.5517, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5496\n",
            "Epoch 143/200, Loss: 0.5517, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5495\n",
            "Epoch 144/200, Loss: 0.5512, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5493\n",
            "Epoch 145/200, Loss: 0.5512, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5491\n",
            "Epoch 146/200, Loss: 0.5510, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5489\n",
            "Epoch 147/200, Loss: 0.5501, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5487\n",
            "Epoch 148/200, Loss: 0.5508, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5485\n",
            "Epoch 149/200, Loss: 0.5500, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5484\n",
            "Epoch 150/200, Loss: 0.5498, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5482\n",
            "Epoch 151/200, Loss: 0.5502, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5481\n",
            "Epoch 152/200, Loss: 0.5495, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5480\n",
            "Epoch 153/200, Loss: 0.5499, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5480\n",
            "Epoch 154/200, Loss: 0.5494, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5480\n",
            "Epoch 155/200, Loss: 0.5492, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5480\n",
            "Epoch 156/200, Loss: 0.5510, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5478\n",
            "Epoch 157/200, Loss: 0.5504, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5476\n",
            "Epoch 158/200, Loss: 0.5489, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5475\n",
            "Epoch 159/200, Loss: 0.5490, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5475\n",
            "Epoch 160/200, Loss: 0.5496, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5475\n",
            "Epoch 161/200, Loss: 0.5489, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5475\n",
            "Epoch 162/200, Loss: 0.5495, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5475\n",
            "Epoch 163/200, Loss: 0.5486, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5474\n",
            "Epoch 164/200, Loss: 0.5488, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5474\n",
            "Epoch 165/200, Loss: 0.5489, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5473\n",
            "Epoch 166/200, Loss: 0.5490, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5472\n",
            "Epoch 167/200, Loss: 0.5485, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5471\n",
            "Epoch 168/200, Loss: 0.5485, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5470\n",
            "Epoch 169/200, Loss: 0.5484, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5469\n",
            "Epoch 170/200, Loss: 0.5481, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5468\n",
            "Epoch 171/200, Loss: 0.5480, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5468\n",
            "Epoch 172/200, Loss: 0.5482, Hamming Loss: 0.0000\n",
            "Validation Loss: 0.5467\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-c07e8958ec62>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mepochs_without_improvement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Save the model's state_dict (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mepochs_without_improvement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_text(model, tokenizer, text, device, max_len=128, threshold=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Tokenize input text\n",
        "        encoding = tokenizer(\n",
        "            text,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        # Get predictions\n",
        "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probabilities = output.cpu().numpy()[0]\n",
        "        predicted_labels = [i for i, prob in enumerate(probabilities) if prob > threshold]\n",
        "\n",
        "    return predicted_labels, probabilities\n",
        "\n",
        "# Example Usage\n",
        "texts = [\"What about a sandwich?\", \"example text 1 y\"]\n",
        "for text in texts:\n",
        "  thresholds = [0.3, 0.4, 0.5, 0.6]\n",
        "  for threshold in thresholds:\n",
        "    predicted_labels, probabilities = predict_single_text(model, tokenizer, text, device, threshold=threshold)\n",
        "    print(f\"Threshold: {threshold}, Predicted Labels: {[label_names[i] for i in predicted_labels]}, Probabilities: {probabilities}\")\n",
        "    print(getResponse(predicted_labels, threshold))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikWlY-0DYay4",
        "outputId": "36ce5bfd-a6fb-41ea-c538-f28d8b4a8c9c"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 0.3, Predicted Labels: ['p class'], Probabilities: [0.27584076 0.20777488 0.69839764]\n",
            "p class (trashold: 0.3)\n",
            "Threshold: 0.4, Predicted Labels: ['p class'], Probabilities: [0.27584076 0.20777488 0.69839764]\n",
            "p class (trashold: 0.4)\n",
            "Threshold: 0.5, Predicted Labels: ['p class'], Probabilities: [0.27584076 0.20777488 0.69839764]\n",
            "p class (trashold: 0.5)\n",
            "Threshold: 0.6, Predicted Labels: ['p class'], Probabilities: [0.27584076 0.20777488 0.69839764]\n",
            "p class (trashold: 0.6)\n",
            "Threshold: 0.3, Predicted Labels: ['y class', 'p class'], Probabilities: [0.97213644 0.01106302 0.9802633 ]\n",
            "y class p class (trashold: 0.3)\n",
            "Threshold: 0.4, Predicted Labels: ['y class', 'p class'], Probabilities: [0.97213644 0.01106302 0.9802633 ]\n",
            "y class p class (trashold: 0.4)\n",
            "Threshold: 0.5, Predicted Labels: ['y class', 'p class'], Probabilities: [0.97213644 0.01106302 0.9802633 ]\n",
            "y class p class (trashold: 0.5)\n",
            "Threshold: 0.6, Predicted Labels: ['y class', 'p class'], Probabilities: [0.97213644 0.01106302 0.9802633 ]\n",
            "y class p class (trashold: 0.6)\n"
          ]
        }
      ]
    }
  ]
}